load("C:\\Users\\guomi\\Documents\\Auto\\.RData")
glimpse(Auto)
require(ISLR); require(tidyverse); require(GGally)
require(ggthemes); require(knitr); require(kableExtra)
data('Auto')
colMeans(is.na(Auto))
theme_set(theme_tufte(base_size = 14))
glimpse(Auto)
unique(Auto$cylinders)
table(Auto$cylinders)
unique(Auto$origin)
table(Auto$origin)
Auto <- Auto %>%
    filter(!cylinders %in% c(3,5)) %>%
    mutate(cylinders = factor(cylinders,
                                    ordered = TRUE),
           origin = factor(origin, 
                                 levels = c(1, 2, 3), 
                                 labels = c('American', 'European', 'Japanese')))
kable(Auto %>%
          select(-name, -origin, -cylinders) %>%
          gather(Variable, value) %>%
          group_by(Variable) %>%
          summarize(min = min(value), max = max(value), span = max(value) - min(value)))
save.image("C:\\Users\\guomi\\Documents\\Auto\\.RData")
q()
unique(Auto$origin)
glimpse(Auto)
load("C:\\Users\\guomi\\Documents\\Auto\\.RData")
require(ISLR); require(tidyverse); require(GGally)
require(ggthemes); require(knitr); require(kableExtra)
data('Auto')
colMeans(is.na(Auto))
theme_set(theme_tufte(base_size = 14))
glimpse(Auto)
unique(Auto$cylinders)
table(Auto$cylinders)
unique(Auto$origin)
table(Auto$origin)
Auto <- Auto %>%
    filter(!cylinders %in% c(3,5)) %>%
    mutate(cylinders = factor(cylinders,
                                    ordered = TRUE),
           origin = factor(origin, 
                                 levels = c(1, 2, 3), 
                                 labels = c('American', 'European', 'Japanese')))
kable(Auto %>%
          select(-name, -origin, -cylinders) %>%
          gather(Variable, value) %>%
          group_by(Variable) %>%
          summarize(min = min(value), max = max(value), span = max(value) - min(value)))
kable(Auto %>%
          select(-name, -origin, -cylinders) %>%
          gather(Variable, value) %>%
          group_by(Variable) %>%
          summarize(Mean = mean(value), SD = sd(value)),
      digits = 2)
kable(Auto %>%
          slice(-(10:85)) %>%
          gather(Variable, value, -origin, -name, -cylinders) %>%
          group_by(Variable) %>%
          summarize(min = min(value), 
                    max = max(value), 
                    span = max(value) - min(value), 
                    Mean = mean(value),
                    SD = sd(value)),
kable(Auto %>%
          slice(-(10:85)) %>%
          gather(Variable, value, -origin, -name, -cylinders) %>%
          group_by(Variable) %>%
          summarize(min = min(value), 
                    max = max(value), 
                    span = max(value) - min(value), 
                    Mean = mean(value),
                    SD = sd(value)),
      digits = 2)
pairs(Auto)
cor(subset(Auto, select = -name))
model1 <-  lm(mpg ~ .-name, data=Auto)
summary(model1)
q()
data <- read.csv("Boston-1.csv")
original_response <- data$crim
median_response <- median(original_response)
response <- ifelse(original_response > median_response, 1, 0)
predictors <- colnames(data)[2:14]
significant_predictors <- c()
for (predictor in predictors) {
  model <- glm(response ~ data[[predictor]], family = binomial)
  # Check the p-value
  p_value <- summary(model)$coefficients[2, 4]  # Index 2 corresponds to the predictor variable
  if (p_value < 0.05) {
    significant_predictors <- c(significant_predictors, predictor)
  }
  cat(paste(predictor, ": p-value =", p_value, "\n"))
}
cat("Predictors with significant association:", paste(significant_predictors, collapse = ", "), "\n")
model1 <- glm(response ~ data$zn, family = binomial)
summary(model1)
summary(model1)$coefficients
model1 <- glm(response ~ data$zn, family = binomial(link  = "logit"))
summary(model1)
summary(model1)$coefficients
median_response
model <- glm(response ~ ., predictors, family = binomial)
model <- glm(response ~ ., data = predictors, family = binomial)
model <- glm(response ~ ., data = data, family = binomial)
model <- glm(response ~ ., data = data[, 2:14], family = binomial)
summary(model)
new_data <- data.frame(
  zn = 18,
  indus = 2.97,
  chas = 0,
  nox = 0.4,
  rm = 6.575,
  age = 63,
  dis = 4.8,
  rad = 3,
  tax = 238,
  ptratio = 15.3,
  black = 376.7,
  lstat = 8.23,
  medv = 45
)
predicted_prob <- predict(model, newdata = new_data, type = "response")
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
cat("Predicted Probability:", predicted_prob, "\n")
cat("Predicted Class (1: Above Median, 0: Below Median):", predicted_class, "\n")
actual_class <- response
conf_matrix <- table(Actual = actual_class, Predicted = predicted_class)
summary(model)
predicted_prob_all <- predict(model, type = "response")
predicted_prob <- predict(model, type = "response")
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
actual_class <- response
conf_matrix <- table(Actual = actual_class, Predicted = predicted_class)
print("Confusion Matrix:")
print(conf_matrix)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", accuracy, "\n")
sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
specificity <- conf_matrix[1, 1] / sum(conf_matrix[1, ])
balanced_accuracy <- (sensitivity + specificity) / 2
cat("Balanced Accuracy:", balanced_accuracy, "\n")
fp <- conf_matrix[1, 2]
cat("False Positives (FP):", fp, "\n")
fpr <- fp / sum(conf_matrix[1, ])
cat("False Positive Rate (FPR):", fpr, "\n")
library(ROCR)
install.packages("ROCR")
library(ROCR)
prediction <- prediction(predicted_prob, actual_class)
roc <- performance(prediction, "tpr", "fpr")
plot(roc, col = "blue", main = "ROC Curve", lwd = 2)
auc <- performance(prediction, "auc")@y.values[[1]]
cat("AUC:", auc, "\n")
result_df <- data.frame(predicted_prob, actual_class)
thresholds <- seq(0, 1, by = 0.01)
max_sum_rate <- 0
best_threshold <- 0
for (threshold in thresholds) {
  predicted_class <- ifelse(predicted_prob > threshold, 1, 0)
  # Confusion Matrix
  conf_matrix <- table(Actual = actual_class, Predicted = predicted_class)
  # Calculate TP rate and FP rate
  tp_rate <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
  fp_rate <- conf_matrix[1, 2] / sum(conf_matrix[1, ])
  # Calculate the sum of TP rate and (1 - FP rate)
  sum_rate <- tp_rate + (1 - fp_rate)
  # Check if the current sum_rate is higher than the maximum
  if (sum_rate > max_sum_rate) {
    max_sum_rate <- sum_rate
    best_threshold <- threshold
  }
}
threshold
conf_matrix
conf_matrix[1,1]
conf_matrix[2,2]
predicted_class
thresholds <- seq(0.3, 1, by = 0.01)
for (threshold in thresholds) {
  predicted_class <- ifelse(predicted_prob > threshold, 1, 0)
  # Confusion Matrix
  conf_matrix <- table(Actual = actual_class, Predicted = predicted_class)
  # Calculate TP rate and FP rate
  tp_rate <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
  fp_rate <- conf_matrix[1, 2] / sum(conf_matrix[1, ])
  # Calculate the sum of TP rate and (1 - FP rate)
  sum_rate <- tp_rate + (1 - fp_rate)
  # Check if the current sum_rate is higher than the maximum
  if (sum_rate > max_sum_rate) {
    max_sum_rate <- sum_rate
    best_threshold <- threshold
  }
}
conf_matrix
threshold
thresholds <- seq(0.01, 0.99, by = 0.01)
for (threshold in thresholds) {
  predicted_class <- ifelse(predicted_prob > threshold, 1, 0)
  # Confusion Matrix
  conf_matrix <- table(Actual = actual_class, Predicted = predicted_class)
  # Calculate TP rate and FP rate
  tp_rate <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
  fp_rate <- conf_matrix[1, 2] / sum(conf_matrix[1, ])
  # Calculate the sum of TP rate and (1 - FP rate)
  sum_rate <- tp_rate + (1 - fp_rate)
  # Check if the current sum_rate is higher than the maximum
  if (sum_rate > max_sum_rate) {
    max_sum_rate <- sum_rate
    best_threshold <- threshold
  }
}
cat("Best Threshold:", best_threshold, "\n")
result_df <- data.frame(predicted_prob, actual_class)
thresholds <- seq(0.01, 0.99, by = 0.01)
max_sum_rate <- 0
best_threshold <- 0
for (threshold in thresholds) {
  predicted_class <- ifelse(predicted_prob > threshold, 1, 0)
  # Confusion Matrix
  conf_matrix <- table(Actual = actual_class, Predicted = predicted_class)
  # Calculate TP rate and FP rate
  tp_rate <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
  fp_rate <- conf_matrix[1, 2] / sum(conf_matrix[1, ])
  # Calculate the sum of TP rate and (1 - FP rate)
  sum_rate <- tp_rate + (1 - fp_rate)
  # Check if the current sum_rate is higher than the maximum
  if (sum_rate > max_sum_rate) {
    max_sum_rate <- sum_rate
    best_threshold <- threshold
  }
}
cat("Best Threshold:", best_threshold, "\n")
# Apply the best threshold to get the predicted class
predicted_class_best_threshold <- ifelse(predicted_prob > best_threshold, 1, 0)
conf_matrix_best_threshold <- table(Actual = actual_class_all, Predicted = predicted_class_best_threshold)
print("Confusion Matrix:")
conf_matrix_best_threshold <- table(Actual = actual_class_all, Predicted = predicted_class_best_threshold)
conf_matrix_best_threshold <- table(Actual = actual_class, Predicted = predicted_class_best_threshold)
predicted_class_best_threshold <- ifelse(predicted_prob > best_threshold, 1, 0)
conf_matrix_best_threshold <- table(Actual = actual_class, Predicted = predicted_class_best_threshold)
print("Confusion Matrix:")
print(conf_matrix_best_threshold)
accuracy_best_threshold <- sum(diag(conf_matrix_best_threshold)) / sum(conf_matrix_best_threshold)
cat("Accuracy:", accuracy_best_threshold, "\n")
sensitivity_best_threshold <- conf_matrix_best_threshold[2, 2] / sum(conf_matrix_best_threshold[2, ])
specificity_best_threshold <- conf_matrix_best_threshold[1, 1] / sum(conf_matrix_best_threshold[1, ])
balanced_accuracy_best_threshold <- (sensitivity_best_threshold + specificity_best_threshold) / 2
cat("Balanced Accuracy:", balanced_accuracy_best_threshold, "\n")
fp_best_threshold <- conf_matrix_best_threshold[1, 2]
cat("False Positives (FP):", fp_best_threshold, "\n")
fpr_best_threshold <- fp_best_threshold / sum(conf_matrix_best_threshold[1, ])
cat("False Positive Rate (FPR):", fpr_best_threshold, "\n")
distances <- sqrt((1 - roc@tp[[1]] / sum(roc@table)) ^ 2 + (roc@fp[[1]] / sum(roc@table)) ^ 2)
library(ROCR)
prediction <- prediction(predicted_prob, actual_class)
roc <- performance(prediction, "tpr", "fpr")
auc <- performance(prediction, "auc")@y.values[[1]]
cat("AUC:", auc, "\n")
distances <- sqrt((1 - roc@tp[[1]] / sum(roc@table)) ^ 2 + (roc@fp[[1]] / sum(roc@table)) ^ 2)
roc@tp[[1]]
roc@tp[1]
roc@tp
distances <- sqrt((1 - roc@y.values[[1]])^2 + roc@x.values[[1]]^2)
min_distance_index <- which.min(distances)
best_threshold <- roc@alpha.values[[1]][min_distance_index]
cat("Best Threshold:", best_threshold, "\n")
predicted_class_best_threshold <- ifelse(predicted_prob_all > best_threshold, 1, 0)
conf_matrix_best_threshold <- table(Actual = actual_class, Predicted = predicted_class_best_threshold)
print("Confusion Matrix:")
print(conf_matrix_best_threshold)
accuracy_best_threshold <- sum(diag(conf_matrix_best_threshold)) / sum(conf_matrix_best_threshold)
cat("Accuracy:", accuracy_best_threshold, "\n")
sensitivity_best_threshold <- conf_matrix_best_threshold[2, 2] / sum(conf_matrix_best_threshold[2, ])
specificity_best_threshold <- conf_matrix_best_threshold[1, 1] / sum(conf_matrix_best_threshold[1, ])
balanced_accuracy_best_threshold <- (sensitivity_best_threshold + specificity_best_threshold) / 2
cat("Balanced Accuracy:", balanced_accuracy_best_threshold, "\n")
fp_best_threshold <- conf_matrix_best_threshold[1, 2]
cat("False Positives (FP):", fp_best_threshold, "\n")
fpr_best_threshold <- fp_best_threshold / sum(conf_matrix_best_threshold[1, ])
cat("False Positive Rate (FPR):", fpr_best_threshold, "\n")
summary(model)
coefficients <- coef(model)
variable_names <- names(coefficients)
sorted_variables <- variable_names[order(abs(coefficients), decreasing = TRUE)]
cat("Most important variable:", sorted_variables[1], "\n")
cat("Second most important variable:", sorted_variables[2], "\n")
coefficients <- coef(model)
variable_names <- names(coefficients)
variable_names <- variable_names[-1]
variable_names
coefficients <- coef(model)
variable_names <- names(coefficients)
variable_names <- variable_names[-1]
sorted_variables <- variable_names[order(abs(coefficients[-1]), decreasing = TRUE)]
cat("Most important variable:", sorted_variables[1], "\n")
cat("Second most important variable:", sorted_variables[2], "\n")
most_important_var1 <- sorted_variables[1]
most_important_var2 <- sorted_variables[2]
model_most_important <- glm(response ~ get(most_important_var1) + get(most_important_var2), data = data, family = binomial)
summary(model_most_important)
model_most_important <- glm(response ~ get(most_important_var1) + get(most_important_var2), data = data[,2:14], family = binomial)
summary(model_most_important)
most_important_var1 <- sorted_variables[1]
most_important_var2 <- sorted_variables[2]
model_most_important <- glm(response ~ get(most_important_var1) + get(most_important_var2), data = data, family = binomial)
summary(model_most_important)
coefficients <- coef(model_most_important)
cat("Logistic Regression Equation:\n")
cat("log(p / (1 - p)) =", coefficients[1], "+", coefficients[2], "*", most_important_var1, "+", coefficients[3], "*", most_important_var2, "\n")
cat("p =", "1 / (1 + exp(-(", coefficients[1], "+", coefficients[2], "*", most_important_var1, "+", coefficients[3], "*", most_important_var2, ")))\n")
most_important_var1
most_important_var2
model_most_important <- glm(response ~ get(most_important_var1) + get(most_important_var2), data = data, family = binomial)
predicted_prob_most_important <- predict(model_most_important, type = "response")
library(pROC)
install.packages(pROC)
install.packages(PROC)
model_most_important <- glm(response ~ get(most_important_var1) + get(most_important_var2), data = data, family = binomial)
predicted_prob_most_important <- predict(model_most_important, type = "response")
roc_most_important <- roc(actual_class_all, predicted_prob_most_important)
library(ROCR)
roc_most_important <- roc(actual_class_all, predicted_prob_most_important)
model_most_important <- glm(response ~ get(most_important_var1) + get(most_important_var2), data = data, family = binomial)
predicted_prob_most_important <- predict(model_most_important, type = "response")
install.packages("pROC")
model_most_important <- glm(response ~ get(most_important_var1) + get(most_important_var2), data = data, family = binomial)
predicted_prob_most_important <- predict(model_most_important, type = "response")
library(pROC)
roc_most_important <- roc(actual_class_all, predicted_prob_most_important)
model_most_important <- glm(response ~ get(most_important_var1) + get(most_important_var2), data = data, family = binomial)
predicted_prob_most_important <- predict(model_most_important, type = "response")
library(pROC)
roc_most_important <- roc(actual_class, predicted_prob_most_important)
plot(roc_most_important, main = "ROC Curve", col = "blue", lwd = 2)
auc_most_important <- auc(roc_most_important)
cat("AUC:", auc_most_important, "\n")
thresholds <- seq(0.01, 0.99, by = 0.01)
max_sum_rate <- 0
best_threshold <- 0
for (threshold in thresholds) {
  predicted_class <- ifelse(predicted_prob_most_important > threshold, 1, 0)
  # Confusion Matrix
  conf_matrix <- table(Actual = actual_class, Predicted = predicted_class)
  # Calculate TP rate and FP rate
  tp_rate <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
  fp_rate <- conf_matrix[1, 2] / sum(conf_matrix[1, ])
  # Calculate the sum of TP rate and (1 - FP rate)
  sum_rate <- tp_rate + (1 - fp_rate)
  # Check if the current sum_rate is higher than the maximum
  if (sum_rate > max_sum_rate) {
    max_sum_rate <- sum_rate
    best_threshold <- threshold
  }
}
cat("Best Threshold:", best_threshold, "\n")
predicted_class_best_threshold <- ifelse(predicted_prob_most_important > best_threshold, 1, 0)
conf_matrix_best_threshold <- table(Actual = actual_class, Predicted = predicted_class_best_threshold)
print("Confusion Matrix:")
print(conf_matrix_best_threshold)
accuracy_best_threshold <- sum(diag(conf_matrix_best_threshold)) / sum(conf_matrix_best_threshold)
cat("Accuracy:", accuracy_best_threshold, "\n")
sensitivity_best_threshold <- conf_matrix_best_threshold[2, 2] / sum(conf_matrix_best_threshold[2, ])
specificity_best_threshold <- conf_matrix_best_threshold[1, 1] / sum(conf_matrix_best_threshold[1, ])
balanced_accuracy_best_threshold <- (sensitivity_best_threshold + specificity_best_threshold) / 2
cat("Balanced Accuracy:", balanced_accuracy_best_threshold, "\n")
fp_best_threshold <- conf_matrix_best_threshold[1, 2]
cat("False Positives (FP):", fp_best_threshold, "\n")
fpr_best_threshold <- fp_best_threshold / sum(conf_matrix_best_threshold[1, ])
cat("False Positive Rate (FPR):", fpr_best_threshold, "\n")
save.image("C:\\Users\\guomi\\Documents\\Auto\\.RData")
q()
